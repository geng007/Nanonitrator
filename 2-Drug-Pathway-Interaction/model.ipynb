{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安装完成\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install pgl\n",
    "clear_output()\n",
    "!pip install /home/aistudio/work/rdkit_pypi-2021.3.2-cp37-cp37m-manylinux1_x86_64.whl\n",
    "clear_output()\n",
    "print(\"安装完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"./Transformer/PaddleHelix-dev/\")\n",
    "os.chdir(\"./Transformer/PaddleHelix-dev/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2022-03-15 09:54:39,672 [mp_reader.py:   20]:\tWe set multiprocessing start method as 'fork' by default.\n",
      "[INFO] 2022-03-15 09:54:39,675 [mp_reader.py:   25]:\tujson not install, fail back to use json instead\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.distributed as dist\n",
    "import pgl\n",
    "\n",
    "from pahelix.model_zoo.pretrain_gnns_model import PretrainGNNModel, AttrmaskModel\n",
    "from pahelix.datasets.zinc_dataset import load_zinc_dataset\n",
    "from pahelix.utils.splitters import RandomSplitter\n",
    "from pahelix.featurizers.pretrain_gnn_featurizer import AttrmaskTransformFn, AttrmaskCollateFn\n",
    "from pahelix.utils import load_json_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PretrainGNNModel] embed_dim:300\n",
      "[PretrainGNNModel] dropout_rate:0.5\n",
      "[PretrainGNNModel] norm_type:batch_norm\n",
      "[PretrainGNNModel] graph_norm:False\n",
      "[PretrainGNNModel] residual:False\n",
      "[PretrainGNNModel] layer_num:5\n",
      "[PretrainGNNModel] gnn_type:gin\n",
      "[PretrainGNNModel] JK:last\n",
      "[PretrainGNNModel] readout:mean\n",
      "[PretrainGNNModel] atom_names:['atomic_num', 'chiral_tag']\n",
      "[PretrainGNNModel] bond_names:['bond_dir', 'bond_type']\n"
     ]
    }
   ],
   "source": [
    "compound_encoder_config = load_json_config(\"model_configs/pregnn_paper.json\")\n",
    "model_config = load_json_config(\"model_configs/pre_Attrmask.json\")\n",
    "\n",
    "compound_encoder = PretrainGNNModel(compound_encoder_config)\n",
    "model = AttrmaskModel(model_config, compound_encoder)\n",
    "opt = paddle.optimizer.Adam(0.001, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset num: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\reader\\decorator.py\", line 346, in read_worker\n",
      "    for d in r:\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\site-packages\\pgl\\utils\\mp_reader.py\", line 152, in pipe_reader\n",
      "    p.start()\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\multiprocessing\\context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"D:\\Anaconda3\\envs\\paddle\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "AttributeError: Can't pickle local object 'multiprocess_reader.<locals>._read_into_pipe'\n"
     ]
    }
   ],
   "source": [
    "### Load the first 1000 of the toy dataset for speed up\n",
    "dataset = load_zinc_dataset(\"./chem_dataset_small/zinc_standard_agent/\")\n",
    "dataset = dataset[:1000]\n",
    "print(\"dataset num: %s\" % (len(dataset)))\n",
    "\n",
    "transform_fn = AttrmaskTransformFn()\n",
    "dataset.transform(transform_fn, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:648: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 train/loss:2.8609824\n",
      "epoch:1 train/loss:0.96538025\n"
     ]
    }
   ],
   "source": [
    "def train(model, dataset, collate_fn, opt):\n",
    "    data_gen = dataset.get_data_loader(\n",
    "            batch_size=128, \n",
    "            num_workers=4, \n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn)\n",
    "    list_loss = []\n",
    "    model.train()\n",
    "    for graphs, masked_node_indice, masked_node_label in data_gen:\n",
    "        graphs = graphs.tensor()\n",
    "        masked_node_indice = paddle.to_tensor(masked_node_indice, 'int64')\n",
    "        masked_node_label = paddle.to_tensor(masked_node_label, 'int64')\n",
    "        loss = model(graphs, masked_node_indice, masked_node_label)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        list_loss.append(loss.numpy())\n",
    "    return np.mean(list_loss)\n",
    "\n",
    "collate_fn = AttrmaskCollateFn(\n",
    "        atom_names=compound_encoder_config['atom_names'], \n",
    "        bond_names=compound_encoder_config['bond_names'],\n",
    "        mask_ratio=0.15)\n",
    "\n",
    "for epoch_id in range(2):\n",
    "    train_loss = train(model, dataset, collate_fn, opt)\n",
    "    print(\"epoch:%d train/loss:%s\" % (epoch_id, train_loss))\n",
    "paddle.save(compound_encoder.state_dict(), \n",
    "        './model/pretrain_attrmask/compound_encoder.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pahelix.utils.splitters import \\\n",
    "    RandomSplitter, IndexSplitter, ScaffoldSplitter\n",
    "from pahelix.datasets import *\n",
    "\n",
    "from src.model import DownstreamModel\n",
    "from src.featurizer import DownstreamTransformFn, DownstreamCollateFn\n",
    "from src.utils import calc_rocauc_score, exempt_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n"
     ]
    }
   ],
   "source": [
    "task_names = get_default_tox21_task_names()\n",
    "# task_names = get_default_sider_task_names()\n",
    "print(task_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PretrainGNNModel] embed_dim:300\n",
      "[PretrainGNNModel] dropout_rate:0.5\n",
      "[PretrainGNNModel] norm_type:batch_norm\n",
      "[PretrainGNNModel] graph_norm:False\n",
      "[PretrainGNNModel] residual:False\n",
      "[PretrainGNNModel] layer_num:5\n",
      "[PretrainGNNModel] gnn_type:gin\n",
      "[PretrainGNNModel] JK:last\n",
      "[PretrainGNNModel] readout:mean\n",
      "[PretrainGNNModel] atom_names:['atomic_num', 'chiral_tag']\n",
      "[PretrainGNNModel] bond_names:['bond_dir', 'bond_type']\n"
     ]
    }
   ],
   "source": [
    "compound_encoder_config = load_json_config(\"model_configs/pregnn_paper.json\")\n",
    "model_config = load_json_config(\"model_configs/down_linear.json\")\n",
    "model_config['num_tasks'] = len(task_names)\n",
    "\n",
    "compound_encoder = PretrainGNNModel(compound_encoder_config)\n",
    "model = DownstreamModel(model_config, compound_encoder)\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "opt = paddle.optimizer.Adam(0.001, parameters=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compound_encoder.set_state_dict(paddle.load('./model/pretrain_attrmask/compound_encoder.pdparams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [21:21:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:21:11] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [21:21:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[21:21:25] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test num: 6264/783/784\n"
     ]
    }
   ],
   "source": [
    "dataset.transform(DownstreamTransformFn(), num_workers=4)\n",
    "\n",
    "# splitter = RandomSplitter()\n",
    "splitter = ScaffoldSplitter()\n",
    "train_dataset, valid_dataset, test_dataset = splitter.split(\n",
    "        dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1)\n",
    "print(\"Train/Valid/Test num: %s/%s/%s\" % (\n",
    "        len(train_dataset), len(valid_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid ratio: 0.7603235\n",
      "Task evaluated: 12/12\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataset, collate_fn, criterion, opt):\n",
    "    data_gen = train_dataset.get_data_loader(\n",
    "            batch_size=128, \n",
    "            num_workers=4, \n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn)\n",
    "    list_loss = []\n",
    "    model.train()\n",
    "    for graphs, valids, labels in data_gen:\n",
    "        graphs = graphs.tensor()\n",
    "        labels = paddle.to_tensor(labels, 'float32')\n",
    "        valids = paddle.to_tensor(valids, 'float32')\n",
    "        preds = model(graphs)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss = paddle.sum(loss * valids) / paddle.sum(valids)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.clear_grad()\n",
    "        list_loss.append(loss.numpy())\n",
    "    return np.mean(list_loss)\n",
    "\n",
    "def evaluate(model, test_dataset, collate_fn):\n",
    "    data_gen = test_dataset.get_data_loader(\n",
    "            batch_size=128, \n",
    "            num_workers=4, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn)\n",
    "    total_pred = []\n",
    "    total_label = []\n",
    "    total_valid = []\n",
    "    model.eval()\n",
    "    for graphs, valids, labels in data_gen:\n",
    "        graphs = graphs.tensor()\n",
    "        labels = paddle.to_tensor(labels, 'float32')\n",
    "        valids = paddle.to_tensor(valids, 'float32')\n",
    "        preds = model(graphs)\n",
    "        total_pred.append(preds.numpy())\n",
    "        total_valid.append(valids.numpy())\n",
    "        total_label.append(labels.numpy())\n",
    "    total_pred = np.concatenate(total_pred, 0)\n",
    "    total_label = np.concatenate(total_label, 0)\n",
    "    total_valid = np.concatenate(total_valid, 0)\n",
    "    return calc_rocauc_score(total_label, total_pred, total_valid)\n",
    "\n",
    "collate_fn = DownstreamCollateFn(\n",
    "        atom_names=compound_encoder_config['atom_names'], \n",
    "        bond_names=compound_encoder_config['bond_names'])\n",
    "for epoch_id in range(4):\n",
    "    train_loss = train(model, train_dataset, collate_fn, criterion, opt)\n",
    "    val_auc = evaluate(model, valid_dataset, collate_fn)\n",
    "    test_auc = evaluate(model, test_dataset, collate_fn)\n",
    "    print(\"epoch:%s train/loss:%s\" % (epoch_id, train_loss))\n",
    "    print(\"epoch:%s val/auc:%s\" % (epoch_id, val_auc))\n",
    "    print(\"epoch:%s test/auc:%s\" % (epoch_id, test_auc))\n",
    "paddle.save(model.state_dict(), './model/tox21/model.pdparams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compound_encoder_config = load_json_config(\"model_configs/pregnn_paper.json\")\n",
    "model_config = load_json_config(\"model_configs/down_linear.json\")\n",
    "model_config['num_tasks'] = len(task_names)\n",
    "\n",
    "compound_encoder = PretrainGNNModel(compound_encoder_config)\n",
    "model = DownstreamModel(model_config, compound_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.set_state_dict(paddle.load('./model/model.pdparams'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SMILES=\"\"\n",
    "transform_fn = DownstreamTransformFn(is_inference=True)\n",
    "collate_fn = DownstreamCollateFn(\n",
    "        atom_names=compound_encoder_config['atom_names'], \n",
    "        bond_names=compound_encoder_config['bond_names'],\n",
    "        is_inference=True)\n",
    "graph = collate_fn([transform_fn({'smiles': SMILES})])\n",
    "preds = model(graph.tensor()).numpy()[0]\n",
    "print('SMILES:%s' % SMILES)\n",
    "print('Predictions:')\n",
    "for name, prob in zip(task_names, preds):\n",
    "    print(\"  %s:\\t%s\" % (name, prob))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a5e225a9e712771938eb0ad373f9920e6de4e9f4268a66c45b2cbc41fe92e30b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
